{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DEFINE GRAPH\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from utils import ArgsProxy\n",
    "import utils\n",
    "\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "from helpers import process_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "rgb_path = '../data/jpg_128_128_2fps/'\n",
    "of_path = '../data/jpg_128_128_16fps_OF_magnitude_cv2/'\n",
    "\n",
    "# Hyperparameters\n",
    "input_width = 128\n",
    "input_height = 128\n",
    "seq_length = 10\n",
    "seq_stride = 10\n",
    "batch_size = 1\n",
    "COLOR = True\n",
    "nb_labels = 2\n",
    "\n",
    "# Data augmentation\n",
    "aug_flip = 0\n",
    "aug_crop = 0\n",
    "aug_light = 0\n",
    "\n",
    "nb_input_dims =5\n",
    "\n",
    "subject_ids = pd.read_csv('../metadata/horse_subjects.csv')['Subject'].values\n",
    "channels = 3\n",
    "\n",
    "\n",
    "args = ArgsProxy(rgb_path, of_path, input_height, input_width,\n",
    "                 seq_length, seq_stride, batch_size, nb_labels,\n",
    "                 aug_flip, aug_crop, aug_light, nb_input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_handler as dathand\n",
    "dh = dathand.DataHandler(path=args.data_path,\n",
    "                         of_path=args.of_path,\n",
    "                         clip_list_file='videos_overview_missingremoved.csv',\n",
    "                         data_columns=['Pain'],  # Here one can append f. ex. 'Observer'\n",
    "                         image_size=(args.input_height, args.input_width),\n",
    "                         seq_length=args.seq_length,\n",
    "                         seq_stride=args.seq_stride,\n",
    "                         batch_size=args.batch_size,\n",
    "                         color=COLOR,\n",
    "                         nb_labels=args.nb_labels,\n",
    "                         aug_flip=args.aug_flip,\n",
    "                         aug_crop=args.aug_crop,\n",
    "                         aug_light=args.aug_light,\n",
    "                         nb_input_dims=args.nb_input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/jpg_128_128_2fps/\n",
      "../data/jpg_128_128_2fps/\n",
      "../data/jpg_128_128_2fps/\n",
      "../data/jpg_128_128_2fps/\n",
      "../data/jpg_128_128_2fps/\n",
      "../data/jpg_128_128_2fps/\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "subject_dfs = utils.read_or_create_subject_dfs(dh, args, subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_dfs = utils.read_or_create_subject_rgb_and_OF_dfs(dh, args, subject_ids, subject_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an example sequence (random from fifth subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose subject  4\n",
      "Start index in subject dataframe:  5918\n"
     ]
    }
   ],
   "source": [
    "sequence_df = utils.get_sequence(args=args, subject_dfs=subject_dfs, subject=4)\n",
    "y = sequence_df['Pain'].values\n",
    "image_paths = sequence_df['Path'].values\n",
    "of_paths = sequence_df['OF_Path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Video_ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Observer</th>\n",
       "      <th>Train</th>\n",
       "      <th>OF_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5918</td>\n",
       "      <td>5918</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000307.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002449.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5919</td>\n",
       "      <td>5919</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000308.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002457.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5920</td>\n",
       "      <td>5920</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000309.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002465.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5921</td>\n",
       "      <td>5921</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000310.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002473.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5922</td>\n",
       "      <td>5922</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000311.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002481.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5923</td>\n",
       "      <td>5923</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000312.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002489.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5924</td>\n",
       "      <td>5924</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000313.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002497.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5925</td>\n",
       "      <td>5925</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000314.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002505.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5926</td>\n",
       "      <td>5926</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000315.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002513.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5927</td>\n",
       "      <td>5927</td>\n",
       "      <td>5_2b</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_5/5_2b/frame_000316.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002521.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1 Video_ID  \\\n",
       "0        5918          5918     5_2b   \n",
       "1        5919          5919     5_2b   \n",
       "2        5920          5920     5_2b   \n",
       "3        5921          5921     5_2b   \n",
       "4        5922          5922     5_2b   \n",
       "5        5923          5923     5_2b   \n",
       "6        5924          5924     5_2b   \n",
       "7        5925          5925     5_2b   \n",
       "8        5926          5926     5_2b   \n",
       "9        5927          5927     5_2b   \n",
       "\n",
       "                                                  Path  Pain  Observer  Train  \\\n",
       "0  data/jpg_128_128_2fps/horse_5/5_2b/frame_000307.jpg     0         0     -1   \n",
       "1  data/jpg_128_128_2fps/horse_5/5_2b/frame_000308.jpg     0         0     -1   \n",
       "2  data/jpg_128_128_2fps/horse_5/5_2b/frame_000309.jpg     0         0     -1   \n",
       "3  data/jpg_128_128_2fps/horse_5/5_2b/frame_000310.jpg     0         0     -1   \n",
       "4  data/jpg_128_128_2fps/horse_5/5_2b/frame_000311.jpg     0         0     -1   \n",
       "5  data/jpg_128_128_2fps/horse_5/5_2b/frame_000312.jpg     0         0     -1   \n",
       "6  data/jpg_128_128_2fps/horse_5/5_2b/frame_000313.jpg     0         0     -1   \n",
       "7  data/jpg_128_128_2fps/horse_5/5_2b/frame_000314.jpg     0         0     -1   \n",
       "8  data/jpg_128_128_2fps/horse_5/5_2b/frame_000315.jpg     0         0     -1   \n",
       "9  data/jpg_128_128_2fps/horse_5/5_2b/frame_000316.jpg     0         0     -1   \n",
       "\n",
       "                                                                OF_Path  \n",
       "0  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002449.jpg  \n",
       "1  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002457.jpg  \n",
       "2  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002465.jpg  \n",
       "3  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002473.jpg  \n",
       "4  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002481.jpg  \n",
       "5  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002489.jpg  \n",
       "6  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002497.jpg  \n",
       "7  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002505.jpg  \n",
       "8  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002513.jpg  \n",
       "9  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_5/5_2b/flow_002521.jpg  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to the best models for each test subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @Maheen, I realized that I renamed these models somewhat in dropbox to remove\n",
    "# the \"CONVfilters_16\" part of the name which doesn't apply, that was just a default setting,\n",
    "# in case you attempt to load these.\n",
    "\n",
    "t0 = '../models/BEST_MODEL_2stream_5d_adadelta_LSTMunits_32_CONVfilters_16_add_v4_t0_4hl_128jpg2fps_seq10_bs8_adadelta_noaug_run1_rerun_for_gradcam.h5'\n",
    "t1 = '../models/BEST_MODEL_2stream_5d_add_adadelta_LSTMunits_32_CONVfilters_16_add_v4_t1_4hl_128jpg2fps_seq10_bs8_MAG_adadelta_noaug_run1_rerun_for_gradcam.h5'\n",
    "t2 = '../models/BEST_MODEL_2stream_5d_add_adadelta_LSTMunits_32_CONVfilters_16_add_v4_t2_4hl_128jpg2fps_seq10_bs8_MAG_adadelta_noaug_run1_rerun_for_gradcam.h5'\n",
    "t3 = '../models/BEST_MODEL_2stream_5d_add_adadelta_LSTMunits_32_CONVfilters_16_add_v4_t3_4hl_128jpg2fps_seq10_bs8_MAG_adadelta_noaug_run1_rerun_gc.h5'\n",
    "t4 = '../models/BEST_MODEL_2stream_5d_add_adadelta_LSTMunits_32_CONVfilters_16_add_v0_t4_4hl_128jpg2fps_seq10_bs8_MAG_adadelta_noaug_run1_rerun_gc.h5'\n",
    "t5 ='../models/BEST_MODEL_2stream_5d_add_adadelta_LSTMunits_32_CONVfilters_16_add_v4_t5_4hl_128jpg2fps_seq10_bs8_MAG_adadelta_noaug_run1_rerun_gc.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_t0 = keras.models.load_model('../models/BEST_MODEL_2stream_5d_adadelta_LSTMunits_32_CONVfilters_16_add_v4_t0_4hl_128jpg2fps_seq10_bs8_adadelta_noaug_run1_rerun_for_gradcam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x136d35da0>,\n",
       " <keras.engine.input_layer.InputLayer at 0x13539fa90>,\n",
       " <keras.engine.sequential.Sequential at 0x136d35e10>,\n",
       " <keras.engine.sequential.Sequential at 0x13657fc88>,\n",
       " <keras.layers.merge.Multiply at 0x136d24048>,\n",
       " <keras.layers.core.Dropout at 0x138fb6828>,\n",
       " <keras.layers.core.Dense at 0x1390be6a0>,\n",
       " <keras.layers.core.Activation at 0x139227160>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t0.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_t1 = keras.models.load_model('../models/BEST_MODEL_2stream_5d_add_adadelta_LSTMunits_32_CONVfilters_16_add_v0_t4_4hl_128jpg2fps_seq10_bs8_MAG_adadelta_noaug_run1_rerun_gc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x136d35518>,\n",
       " <keras.engine.input_layer.InputLayer at 0x136d356a0>,\n",
       " <keras.engine.sequential.Sequential at 0x136d35940>,\n",
       " <keras.engine.sequential.Sequential at 0x140b27240>,\n",
       " <keras.layers.merge.Add at 0x136d357f0>,\n",
       " <keras.layers.core.Dropout at 0x1440eacc0>,\n",
       " <keras.layers.core.Dense at 0x144200a20>,\n",
       " <keras.layers.core.Activation at 0x14435b898>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t1.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional_recurrent.ConvLSTM2D at 0x136d35a90>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x140aa70b8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x141c18a58>,\n",
       " <keras.layers.convolutional_recurrent.ConvLSTM2D at 0x136d35cc0>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x141b94b70>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x140d4e0b8>,\n",
       " <keras.layers.convolutional_recurrent.ConvLSTM2D at 0x13b7324e0>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x140d24048>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x140c144a8>,\n",
       " <keras.layers.convolutional_recurrent.ConvLSTM2D at 0x140c96048>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x140c079b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x140bb5f60>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x140aa7908>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t1.layers[2].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Video_ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Observer</th>\n",
       "      <th>Train</th>\n",
       "      <th>OF_Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_1a</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000001.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1_1a</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000002.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1_1a</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000003.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000017.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1_1a</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000004.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000025.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1_1a</td>\n",
       "      <td>data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000005.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000033.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1 Video_ID  \\\n",
       "0           0             0     1_1a   \n",
       "1           1             1     1_1a   \n",
       "2           2             2     1_1a   \n",
       "3           3             3     1_1a   \n",
       "4           4             4     1_1a   \n",
       "\n",
       "                                                    Path  Pain  Observer  \\\n",
       "0  data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000001.jpg     0         0   \n",
       "1  data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000002.jpg     0         0   \n",
       "2  data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000003.jpg     0         0   \n",
       "3  data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000004.jpg     0         0   \n",
       "4  data/jpg_128_128_2fps/horse_1/1_1a_1/frame_000005.jpg     0         0   \n",
       "\n",
       "   Train  \\\n",
       "0     -1   \n",
       "1     -1   \n",
       "2     -1   \n",
       "3     -1   \n",
       "4     -1   \n",
       "\n",
       "                                                                  OF_Path  \n",
       "0  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000001.jpg  \n",
       "1  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000009.jpg  \n",
       "2  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000017.jpg  \n",
       "3  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000025.jpg  \n",
       "4  data/jpg_128_128_16fps_OF_magnitude_cv2/horse_1/1_1a_1/flow_000033.jpg  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_2stream_image_generator_5D(df, train, val, test, evaluate,\n",
    "                                       rgb_period, flow_period):\n",
    "    \"\"\"\n",
    "    Prepare the frames into labeled train and test sets, with help from the\n",
    "    DataFrame with .jpg-paths and labels for train and pain.\n",
    "    :param df: pd.DataFrame\n",
    "    :param train: Boolean\n",
    "    :param val: Boolean\n",
    "    :param test: Boolean\n",
    "    :param evaluate: Boolean\n",
    "    :param rgb_period: int, if 10, take every 10th frame. # Not relevant here\n",
    "    :param flow_period: int, if 1, take every frame.      # Not relevant here\n",
    "    :return: np.ndarray, np.ndarray, np.ndarray, np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    nb_frames = len(df)\n",
    "    print(\"LEN DF (nb. of frames): \", nb_frames)\n",
    "\n",
    "    ws = args.seq_length  # \"Window size\" in a sliding window.\n",
    "    ss = args.seq_stride  # Stride for the extracted windows\n",
    "    valid = nb_frames - (ws - 1)\n",
    "    nw = valid//ss  # Number of windows\n",
    "    print('Number of windows', nw)\n",
    "\n",
    "    this_index = 0\n",
    "    seq_index = 0\n",
    "\n",
    "    # Make sure that no augmented sequences are thrown away,\n",
    "    # because we really want to augment the dataset.\n",
    "\n",
    "    nb_aug = args.aug_flip + args.aug_crop + args.aug_light\n",
    "    batch_requirement = 1 + nb_aug  # Normal sequence plus augmented sequences.\n",
    "    assert (args.batch_size % batch_requirement) == 0\n",
    "\n",
    "    while True:\n",
    "        # Shuffle blocks between epochs.\n",
    "        if train:\n",
    "            df = shuffle_blocks(df, 'Video_ID')\n",
    "        batch_index = 0\n",
    "        for window_index in range(nw):\n",
    "            start = window_index * ss\n",
    "            stop = start + ws\n",
    "            rows = df.iloc[start:stop]  # A new dataframe for the window in question.\n",
    "\n",
    "            X_seq_list = []\n",
    "            y_seq_list = []\n",
    "            flow_seq_list = []\n",
    "\n",
    "            for index, row in rows.iterrows():\n",
    "                vid_seq_name = row['Video_ID']\n",
    "\n",
    "                if this_index == 0:\n",
    "                    print('First frame. Set oldname=vidname')\n",
    "                    old_vid_seq_name = vid_seq_name  # This variable is set once\n",
    "                    this_index += 1\n",
    "\n",
    "                if vid_seq_name != old_vid_seq_name:\n",
    "                    seq_index = 0\n",
    "                    old_vid_seq_name = vid_seq_name\n",
    "                    break  # Skip this one and jump to next window\n",
    "\n",
    "                if (seq_index % rgb_period) == 0:\n",
    "                    x = get_image(args, row['Path'])\n",
    "                    X_seq_list.append(x)\n",
    "                    y = row['Pain']\n",
    "                    y_seq_list.append(y)\n",
    "\n",
    "                if (seq_index % flow_period) == 0:\n",
    "                    flow = get_image(args, row['OF_Path'])\n",
    "                    if rgb_period > 1:\n",
    "                        # We only want the first two channels of the flow.\n",
    "                        flow = np.take(flow, [0,1], axis=2)\n",
    "                    flow_seq_list.append(flow)\n",
    "\n",
    "                seq_index += 1\n",
    "\n",
    "            if batch_index == 0:\n",
    "                X_batch_list = []\n",
    "                y_batch_list = []\n",
    "                flow_batch_list = []\n",
    "\n",
    "            if seq_index == args.seq_length:\n",
    "                if rgb_period > 1:\n",
    "                    flow_seq_list = np.array(flow_seq_list)\n",
    "                    flow_seq_list = np.reshape(np.array(flow_seq_list),\n",
    "                                              (-1, args.image_size[0], args.image_size[1]))\n",
    "                    X_seq_list = np.reshape(np.array(X_seq_list),\n",
    "                                           (args.image_size[0], args.image_size[1], -1))\n",
    "\n",
    "                X_batch_list.append(X_seq_list)\n",
    "                y_batch_list.append(y_seq_list)\n",
    "                flow_batch_list.append(flow_seq_list)\n",
    "                batch_index += 1\n",
    "                seq_index = 0\n",
    "\n",
    "                if train and (args.aug_flip == 1):\n",
    "                    # Flip both RGB and flow arrays\n",
    "                    X_seq_list_flipped = args.flip_images(X_seq_list)\n",
    "                    flow_seq_list_flipped = args.flip_images(flow_seq_list)\n",
    "                    # Append to the respective batch lists\n",
    "                    X_batch_list.append(X_seq_list_flipped)\n",
    "                    y_batch_list.append(y_seq_list)\n",
    "                    flow_batch_list.append(flow_seq_list_flipped)\n",
    "                    batch_index += 1\n",
    "\n",
    "                if train and (args.aug_crop == 1):\n",
    "                    crop_size = 99\n",
    "                    # Flip both RGB and flow arrays\n",
    "                    X_seq_list_cropped = args.random_crop_resize(X_seq_list,\n",
    "                                                                 crop_size, crop_size)\n",
    "                    flow_seq_list_cropped = args.random_crop_resize(flow_seq_list,\n",
    "                                                                    crop_size, crop_size)\n",
    "                    # Append to the respective batch lists\n",
    "                    X_batch_list.append(X_seq_list_cropped)\n",
    "                    y_batch_list.append(y_seq_list)\n",
    "                    flow_batch_list.append(flow_seq_list_cropped)\n",
    "                    batch_index += 1\n",
    "\n",
    "                if train and (args.aug_light == 1):\n",
    "                    # Flip both RGB and flow arrays\n",
    "                    X_seq_list_shaded = args.add_gaussian_noise(X_seq_list)\n",
    "                    flow_seq_list_shaded = args.add_gaussian_noise(flow_seq_list)\n",
    "                    # Append to the respective batch lists\n",
    "                    X_batch_list.append(X_seq_list_shaded)\n",
    "                    y_batch_list.append(y_seq_list)\n",
    "                    flow_batch_list.append(flow_seq_list_shaded)\n",
    "                    batch_index += 1\n",
    "\n",
    "            if batch_index % args.batch_size == 0 and not batch_index == 0:\n",
    "                X_array = np.array(X_batch_list, dtype=np.float32)\n",
    "                y_array = np.array(y_batch_list, dtype=np.uint8)\n",
    "                flow_array = np.array(flow_batch_list, dtype=np.float32)\n",
    "                if args.nb_labels == 2:\n",
    "                    y_array = np_utils.to_categorical(y_array, num_classes=args.nb_labels)\n",
    "                    y_array = np.reshape(y_array, (args.batch_size, -1, args.nb_labels))\n",
    "                else:\n",
    "                    y_array = np.reshape(y_array, (args.batch_size, -1, args.nb_labels))\n",
    "                if rgb_period > 1:\n",
    "                    y_array = np.reshape(y_array, (args.batch_size, args.nb_labels))\n",
    "                batch_index = 0\n",
    "                # print(X_array.shape, flow_array.shape, y_array.shape)\n",
    "                yield X_array, flow_array, y_array, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(args, path):\n",
    "    channels = 3 if dh.color else 1\n",
    "    im = process_image('../' + path, (dh.image_size[0], dh.image_size[1], channels))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate flattened features from both RGB and optical flow streams for each of the test subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First subject (t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cf3dbbfb8f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First subject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubject_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get intermediate output from the model (the flattened output of each separate stream)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m    926\u001b[0m                              ' elements.')\n\u001b[1;32m    927\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2438\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2440\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 197\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    198\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py365/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First subject\n",
    "df = subject_dfs[0]\n",
    "model = keras.models.load_model(t0)\n",
    "\n",
    "# Get intermediate output from the model (the flattened output of each separate stream)\n",
    "get_last_rgb_output = K.function([model.layers[2].layers[0].input], [model.layers[2].layers[-1].output])\n",
    "get_last_of_output = K.function([model.layers[3].layers[0].input], [model.layers[3].layers[-1].output])\n",
    "\n",
    "features_t0 = []\n",
    "counter = 0\n",
    "\n",
    "for value in prepare_2stream_image_generator_5D(df,\n",
    "                                                train=False,\n",
    "                                                val=False,\n",
    "                                                test=True,\n",
    "                                                evaluate=True,\n",
    "                                                rgb_period=1,\n",
    "                                                flow_period=1):\n",
    "    counter += 1\n",
    "    if counter > 1431:\n",
    "        break\n",
    "    \n",
    "    start_ind = value[3]\n",
    "    \n",
    "    print(counter, start_ind, end=\"\\r\")\n",
    "    \n",
    "    rgb = value[0]\n",
    "    of = value[1]\n",
    "    \n",
    "    last_rgb_output = get_last_rgb_output([rgb])\n",
    "    last_of_output = get_last_of_output([of])\n",
    "    to_save = [last_rgb_output, last_of_output, start_ind]\n",
    "    features_t0.append(to_save)\n",
    "\n",
    "\n",
    "features_t0 = np.asarray(features_t0)\n",
    "np.savez_compressed('../data/features_t0', features_t0=features_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second subject (t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN DF (nb. of frames):  12118\n",
      "Number of windows 1210\n",
      "First frame. Set oldname=vidname\n",
      "1210 60090\r"
     ]
    }
   ],
   "source": [
    "# Second subject\n",
    "df = subject_dfs[1]\n",
    "model = keras.models.load_model(t1)\n",
    "\n",
    "# Get intermediate output from the model (the flattened output of each separate stream)\n",
    "get_last_rgb_output = K.function([model.layers[2].layers[0].input], [model.layers[2].layers[-1].output])\n",
    "get_last_of_output = K.function([model.layers[3].layers[0].input], [model.layers[3].layers[-1].output])\n",
    "\n",
    "features_t1 = []\n",
    "counter = 0\n",
    "\n",
    "for value in prepare_2stream_image_generator_5D(df,\n",
    "                                                train=False,\n",
    "                                                val=False,\n",
    "                                                test=True,\n",
    "                                                evaluate=True,\n",
    "                                                rgb_period=1,\n",
    "                                                flow_period=1):\n",
    "    counter += 1\n",
    "    if counter > 1210:\n",
    "        break\n",
    "    \n",
    "    start_ind = value[3]\n",
    "    \n",
    "    print(counter, start_ind, end=\"\\r\")\n",
    "    \n",
    "    rgb = value[0]\n",
    "    of = value[1]\n",
    "    \n",
    "    last_rgb_output = get_last_rgb_output([rgb])\n",
    "    last_of_output = get_last_of_output([of])\n",
    "    to_save = [last_rgb_output, last_of_output, start_ind]\n",
    "    features_t1.append(to_save)\n",
    "\n",
    "\n",
    "features_t1 = np.asarray(features_t1)\n",
    "np.savez_compressed('../data/features_t1', features_t1=features_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third subject (t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN DF (nb. of frames):  10924\n",
      "Number of windows 1091\n",
      "First frame. Set oldname=vidname\n",
      "1091 70900\r"
     ]
    }
   ],
   "source": [
    "df = subject_dfs[2]\n",
    "model = keras.models.load_model(t2)\n",
    "\n",
    "# Get intermediate output from the model (the flattened output of each separate stream)\n",
    "get_last_rgb_output = K.function([model.layers[2].layers[0].input], [model.layers[2].layers[-1].output])\n",
    "get_last_of_output = K.function([model.layers[3].layers[0].input], [model.layers[3].layers[-1].output])\n",
    "\n",
    "features_t2 = []\n",
    "counter = 0\n",
    "\n",
    "for value in prepare_2stream_image_generator_5D(df,\n",
    "                                                train=False,\n",
    "                                                val=False,\n",
    "                                                test=True,\n",
    "                                                evaluate=True,\n",
    "                                                rgb_period=1,\n",
    "                                                flow_period=1):\n",
    "    counter += 1\n",
    "    if counter > 1091:\n",
    "        break\n",
    "    \n",
    "    start_ind = value[3]\n",
    "    \n",
    "    print(counter, start_ind, end=\"\\r\")\n",
    "    \n",
    "    rgb = value[0]\n",
    "    of = value[1]\n",
    "    \n",
    "    last_rgb_output = get_last_rgb_output([rgb])\n",
    "    last_of_output = get_last_of_output([of])\n",
    "    to_save = [last_rgb_output, last_of_output, start_ind]\n",
    "    features_t2.append(to_save)\n",
    "\n",
    "\n",
    "features_t2 = np.asarray(features_t2)\n",
    "np.savez_compressed('../data/features_t2', features_t2=features_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth subject (t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN DF (nb. of frames):  13626\n",
      "Number of windows 1361\n",
      "First frame. Set oldname=vidname\n",
      "1361 70600\r"
     ]
    }
   ],
   "source": [
    "df = subject_dfs[3]\n",
    "model = keras.models.load_model(t3)\n",
    "\n",
    "# Get intermediate output from the model (the flattened output of each separate stream)\n",
    "get_last_rgb_output = K.function([model.layers[2].layers[0].input], [model.layers[2].layers[-1].output])\n",
    "get_last_of_output = K.function([model.layers[3].layers[0].input], [model.layers[3].layers[-1].output])\n",
    "\n",
    "features_t3 = []\n",
    "counter = 0\n",
    "\n",
    "for value in prepare_2stream_image_generator_5D(df,\n",
    "                                                train=False,\n",
    "                                                val=False,\n",
    "                                                test=True,\n",
    "                                                evaluate=True,\n",
    "                                                rgb_period=1,\n",
    "                                                flow_period=1):\n",
    "    counter += 1\n",
    "    if counter > 1361:\n",
    "        break\n",
    "    \n",
    "    start_ind = value[3]\n",
    "    \n",
    "    print(counter, start_ind, end=\"\\r\")\n",
    "    \n",
    "    rgb = value[0]\n",
    "    of = value[1]\n",
    "    \n",
    "    last_rgb_output = get_last_rgb_output([rgb])\n",
    "    last_of_output = get_last_of_output([of])\n",
    "    to_save = [last_rgb_output, last_of_output, start_ind]\n",
    "    features_t3.append(to_save)\n",
    "\n",
    "\n",
    "features_t3 = np.asarray(features_t3)\n",
    "np.savez_compressed('../data/features_t3', features_t3=features_t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth subject (t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN DF (nb. of frames):  12534\n",
      "Number of windows 1252\n",
      "First frame. Set oldname=vidname\n",
      "1252 70510\r"
     ]
    }
   ],
   "source": [
    "df = subject_dfs[4]\n",
    "model = keras.models.load_model(t4)\n",
    "\n",
    "# Get intermediate output from the model (the flattened output of each separate stream)\n",
    "get_last_rgb_output = K.function([model.layers[2].layers[0].input], [model.layers[2].layers[-1].output])\n",
    "get_last_of_output = K.function([model.layers[3].layers[0].input], [model.layers[3].layers[-1].output])\n",
    "\n",
    "features_t4 = []\n",
    "counter = 0\n",
    "\n",
    "for value in prepare_2stream_image_generator_5D(df,\n",
    "                                                train=False,\n",
    "                                                val=False,\n",
    "                                                test=True,\n",
    "                                                evaluate=True,\n",
    "                                                rgb_period=1,\n",
    "                                                flow_period=1):\n",
    "    counter += 1\n",
    "    if counter > 1252:\n",
    "        break\n",
    "    \n",
    "    start_ind = value[3]\n",
    "    \n",
    "    print(counter, start_ind, end=\"\\r\")\n",
    "    \n",
    "    rgb = value[0]\n",
    "    of = value[1]\n",
    "    \n",
    "    last_rgb_output = get_last_rgb_output([rgb])\n",
    "    last_of_output = get_last_of_output([of])\n",
    "    to_save = [last_rgb_output, last_of_output, start_ind]\n",
    "    features_t4.append(to_save)\n",
    "\n",
    "\n",
    "features_t4 = np.asarray(features_t4)\n",
    "np.savez_compressed('../data/features_t4', features_t4=features_t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth subject (t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN DF (nb. of frames):  6692\n",
      "Number of windows 668\n",
      "First frame. Set oldname=vidname\n",
      "668 3070\r"
     ]
    }
   ],
   "source": [
    "df = subject_dfs[5]\n",
    "model = keras.models.load_model(t5)\n",
    "\n",
    "# Get intermediate output from the model (the flattened output of each separate stream)\n",
    "get_last_rgb_output = K.function([model.layers[2].layers[0].input], [model.layers[2].layers[-1].output])\n",
    "get_last_of_output = K.function([model.layers[3].layers[0].input], [model.layers[3].layers[-1].output])\n",
    "\n",
    "features_t5 = []\n",
    "counter = 0\n",
    "\n",
    "for value in prepare_2stream_image_generator_5D(df,\n",
    "                                                train=False,\n",
    "                                                val=False,\n",
    "                                                test=True,\n",
    "                                                evaluate=True,\n",
    "                                                rgb_period=1,\n",
    "                                                flow_period=1):\n",
    "    counter += 1\n",
    "    if counter > 668:\n",
    "        break\n",
    "    \n",
    "    start_ind = value[3]\n",
    "    \n",
    "    print(counter, start_ind, end=\"\\r\")\n",
    "    \n",
    "    rgb = value[0]\n",
    "    of = value[1]\n",
    "    \n",
    "    last_rgb_output = get_last_rgb_output([rgb])\n",
    "    last_of_output = get_last_of_output([of])\n",
    "    to_save = [last_rgb_output, last_of_output, start_ind]\n",
    "    features_t5.append(to_save)\n",
    "\n",
    "\n",
    "features_t5 = np.asarray(features_t5)\n",
    "np.savez_compressed('../data/features_t5', features_t5=features_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([array([[[-0.16999894, -0.13054055,  0.23271295, ...,  0.17906334,\n",
      "         -0.3020972 , -0.80027014],\n",
      "        [ 0.14207786, -0.14978519,  0.3541257 , ...,  0.3062452 ,\n",
      "         -0.19731688, -0.7591243 ],\n",
      "        [ 0.5062062 , -0.09573984,  0.3479384 , ...,  0.41385764,\n",
      "         -0.29754314, -0.5495806 ],\n",
      "        ...,\n",
      "        [ 1.5066361 ,  0.4606979 ,  0.23010904, ...,  0.3948507 ,\n",
      "         -0.46377826,  0.19836313],\n",
      "        [ 1.5685604 ,  0.80339056,  0.19359308, ...,  0.3757646 ,\n",
      "         -0.21516004,  0.07680392],\n",
      "        [ 1.5875493 ,  1.1781039 ,  0.14046088, ...,  0.50268376,\n",
      "         -0.02485543, -0.03744203]]], dtype=float32)])\n",
      " list([array([[[-0.24134095,  0.2715644 ,  0.34875235, ..., -0.2629024 ,\n",
      "         -0.84345174, -0.42142418],\n",
      "        [-0.204056  ,  0.13848266,  0.4299632 , ...,  0.01679349,\n",
      "         -0.770345  , -0.28449717],\n",
      "        [-0.0386588 ,  0.00661561,  0.5900829 , ...,  0.27658594,\n",
      "         -0.7818452 , -0.08397353],\n",
      "        ...,\n",
      "        [ 0.7711395 ,  0.518631  ,  1.1219597 , ...,  0.77602154,\n",
      "         -1.0178678 ,  0.41461253],\n",
      "        [ 0.8044195 ,  0.6000804 ,  1.1923274 , ...,  0.7980185 ,\n",
      "         -1.0472636 ,  0.37066007],\n",
      "        [ 0.78644574,  0.67283416,  1.2684314 , ...,  0.8549147 ,\n",
      "         -1.0720752 ,  0.3318894 ]]], dtype=float32)])\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "fs = np.load('../data/features_t0.npz')\n",
    "print(fs['features_t0'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py365)",
   "language": "python",
   "name": "py365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
